import os
import asyncio
import inspect
import logging
import logging.config
from lightrag import LightRAG, QueryParam
from lightrag.llm.ollama import ollama_model_complete, ollama_embed
from lightrag.utils import EmbeddingFunc, logger, set_verbose_debug
from lightrag.kg.shared_storage import initialize_pipeline_status
from docx import Document
from dotenv import load_dotenv

import gradio as gr

load_dotenv(dotenv_path=".env", override=False)

WORKING_DIR = "./dickens"
conversation_history = []
rag = None
initialization_complete = False  # æ·»åŠ åˆå§‹åŒ–çŠ¶æ€æ ‡å¿—


def configure_logging():
    for logger_name in ["uvicorn", "uvicorn.access", "uvicorn.error", "lightrag"]:
        logger_instance = logging.getLogger(logger_name)
        logger_instance.handlers = []
        logger_instance.filters = []

    log_dir = os.getenv("LOG_DIR", os.getcwd())
    log_file_path = os.path.abspath(os.path.join(log_dir, "lightrag_ollama_demo.log"))
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

    log_max_bytes = int(os.getenv("LOG_MAX_BYTES", 10485760))
    log_backup_count = int(os.getenv("LOG_BACKUP_COUNT", 5))

    logging.config.dictConfig({
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "default": {"format": "%(levelname)s: %(message)s"},
            "detailed": {"format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"},
        },
        "handlers": {
            "console": {
                "formatter": "default",
                "class": "logging.StreamHandler",
                "stream": "ext://sys.stderr",
            },
            "file": {
                "formatter": "detailed",
                "class": "logging.handlers.RotatingFileHandler",
                "filename": log_file_path,
                "maxBytes": log_max_bytes,
                "backupCount": log_backup_count,
                "encoding": "utf-8",
            },
        },
        "loggers": {
            "lightrag": {
                "handlers": ["console", "file"],
                "level": "INFO",
                "propagate": False,
            },
        },
    })

    logger.setLevel(logging.INFO)
    set_verbose_debug(os.getenv("VERBOSE_DEBUG", "false").lower() == "true")


def load_docx(docx_path):
    doc = Document(docx_path)
    return "\n".join([para.text for para in doc.paragraphs])


async def load_multiple_docx(docx_paths):
    """æ‰¹é‡åŠ è½½å¤šä¸ªdocxæ–‡æ¡£"""
    texts = []
    for docx_path in docx_paths:
        try:
            text_content = load_docx(docx_path)
            texts.append(text_content)
            print(f"å·²åŠ è½½æ–‡æ¡£: {docx_path}")
            print(f"æ–‡æ¡£å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
        except Exception as e:
            print(f"åŠ è½½æ–‡æ¡£ {docx_path} æ—¶å‡ºé”™: {e}")
    return texts


async def initialize_rag():
    global rag
    
    # æ¸…ç†æ—§æ–‡ä»¶
    files_to_delete = [
        "graph_chunk_entity_relation.graphml",
        "kv_store_doc_status.json",
        "kv_store_full_docs.json",
        "kv_store_text_chunks.json",
        "vdb_chunks.json",
        "vdb_entities.json",
        "vdb_relationships.json",
    ]
    if not os.path.exists(WORKING_DIR):
        os.mkdir(WORKING_DIR)
    for file in files_to_delete:
        file_path = os.path.join(WORKING_DIR, file)
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"Deleting old file: {file_path}")

    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=ollama_model_complete,
        llm_model_name=os.getenv("LLM_MODEL", "qwen2.5:7b"),
        llm_model_max_token_size=8192,
        llm_model_kwargs={
            "host": os.getenv("LLM_BINDING_HOST", "http://localhost:11434"),
            "options": {"num_ctx": 8192},
            "timeout": int(os.getenv("TIMEOUT", "300")),
        },
        embedding_func=EmbeddingFunc(
            embedding_dim=int(os.getenv("EMBEDDING_DIM", "1024")),
            max_token_size=int(os.getenv("MAX_EMBED_TOKENS", "8192")),
            func=lambda texts: ollama_embed(
                texts,
                embed_model=os.getenv("EMBEDDING_MODEL", "bge-m3:latest"),
                host=os.getenv("EMBEDDING_BINDING_HOST", "http://localhost:11434"),
            ),
        ),
        max_parallel_insert=4
    )
    await rag.initialize_storages()
    await initialize_pipeline_status()


async def load_documents():
    # å®šä¹‰å¤šä¸ªæ–‡æ¡£è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
    docx_paths = [
        "./è´¢åŠ¡æ–‡ä»¶/å‡ºå·®äººå‘˜å¸‚å†…äº¤é€šå……å€¼è´¹ç”¨æŠ¥é”€è§„å®šï¼ˆè¯•è¡Œï¼‰.docx",
        "./è´¢åŠ¡æ–‡ä»¶/ä¸œç”µè‚¡è´¢[2014]1å·1ã€æŠ¥é”€ç¥¨æ®è¦æ±‚.docx",
        "./è´¢åŠ¡æ–‡ä»¶/ä¸œç”µè‚¡è´¢[2014]1å·2ã€å›½å†…å·®æ—…è´¹æŠ¥é”€ç¥¨æ®ç²˜è´´æ ‡å‡†.docx",
        "./è´¢åŠ¡æ–‡ä»¶/ä¸œç”µè‚¡è´¢[2014]1å·3ã€æµ·å¤–å·®æ—…è´¹æŠ¥é”€ç¥¨æ®ç²˜è´´æ ‡å‡†.docx",
        "./è´¢åŠ¡æ–‡ä»¶/ä¸œç”µè‚¡è´¢[2014]1å·4ã€éå·®æ—…è´¹æŠ¥é”€ç¥¨æ®ç²˜è´´æ ‡å‡†.docx",
    ]

    # æ‰¹é‡åŠ è½½æ–‡æ¡£
    print("\n=========== å¼€å§‹æ‰¹é‡åŠ è½½æ–‡æ¡£ ===========")
    texts = await load_multiple_docx(docx_paths)
    
    # æ‰¹é‡æ’å…¥æ–‡æ¡£
    if texts:
        print(f"\nå¼€å§‹æ‰¹é‡æ’å…¥ {len(texts)} ä¸ªæ–‡æ¡£...")
        await rag.ainsert(texts)
        print("æ‰¹é‡æ’å…¥å®Œæˆï¼")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")


async def hybrid_query(user_input: str, mode="hybrid", history_turns=30):
    global conversation_history, rag, initialization_complete
    
    if not initialization_complete:
        return "ç³»ç»Ÿæ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™..."
    
    if rag is None:
        return "ç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•ã€‚"

    param = QueryParam(
        mode=mode,
        stream=True,
        conversation_history=conversation_history[-2 * history_turns :],
        history_turns=history_turns,
    )

    conversation_history.append({"role": "user", "content": user_input})
    resp = await rag.aquery(user_input, param=param)

    content = ""
    if inspect.isasyncgen(resp):
        async for chunk in resp:
            content += chunk
    else:
        content = str(resp)

    conversation_history.append({"role": "assistant", "content": content})
    return content


def launch_gradio():
    async def async_wrapper(message, history, mode):
        response = await hybrid_query(message, mode=mode)
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": response})
        return history, history

    async def on_start():
        """Gradio å¯åŠ¨æ—¶çš„å¼‚æ­¥åˆå§‹åŒ–"""
        global initialization_complete
        try:
            print("ğŸš€ å¼€å§‹åˆå§‹åŒ– LightRAG...")
            await initialize_rag()
            await load_documents()
            initialization_complete = True
            print("âœ… åˆå§‹åŒ–å®Œæˆï¼")
            return "ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼"
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            return f"åˆå§‹åŒ–å¤±è´¥: {str(e)}"

    with gr.Blocks() as demo:
        gr.Markdown("# ğŸ“„ LightRAG + Ollama æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
        gr.Markdown("### å·²åŠ è½½å¤šä¸ªè´¢åŠ¡æ–‡æ¡£ï¼Œæ”¯æŒå¤šè½®å¯¹è¯")
        
        # æ·»åŠ çŠ¶æ€æ˜¾ç¤º
        status_text = gr.Textbox(
            value="æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...",
            label="ç³»ç»ŸçŠ¶æ€",
            interactive=False
        )

        chatbot = gr.Chatbot(label="å¯¹è¯å†å²", type="messages")

        with gr.Row():
            msg = gr.Textbox(
                label="è¯·è¾“å…¥é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼šè¿™ä»½æ–‡ä»¶çš„é€‚ç”¨èŒƒå›´æ˜¯ä»€ä¹ˆï¼Ÿ",
                lines=2,
                scale=4,
            )
            send_btn = gr.Button("å‘é€", scale=1)

        with gr.Row():
            mode_dropdown = gr.Dropdown(
                choices=["naive", "local", "global", "hybrid"],
                value="hybrid",
                label="æ£€ç´¢æ¨¡å¼ (Query Mode)",
                interactive=True,
            )

        clear_btn = gr.Button("æ¸…é™¤å†å²")

        def clear_history():
            global conversation_history
            conversation_history = []
            return [], []

        # æ³¨å†Œå¯åŠ¨æ—¶çš„åˆå§‹åŒ–
        demo.load(on_start, None, status_text)
        
        msg.submit(async_wrapper, [msg, chatbot, mode_dropdown], [chatbot, chatbot])
        send_btn.click(async_wrapper, [msg, chatbot, mode_dropdown], [chatbot, chatbot])
        clear_btn.click(fn=clear_history, outputs=[chatbot, chatbot])

    demo.launch(share=True)


if __name__ == "__main__":
    configure_logging()
    launch_gradio()
